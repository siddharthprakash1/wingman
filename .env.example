# LangExtract API Keys and Configuration
# Copy this file to .env and fill in your actual values
# DO NOT commit .env to version control!

# =============================================================================
# GEMINI / GOOGLE AI STUDIO
# =============================================================================
# Get your API key from: https://aistudio.google.com/app/apikey
# This is the primary API key for using Gemini models with LangExtract
LANGEXTRACT_API_KEY=your-gemini-api-key-here

# Alternative: Specific Gemini API key variable
# GEMINI_API_KEY=your-gemini-api-key-here

# =============================================================================
# VERTEX AI (Enterprise/Production)
# =============================================================================
# For Vertex AI authentication, you typically use service account credentials
# via GOOGLE_APPLICATION_CREDENTIALS instead of API keys
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# Vertex AI project configuration (used when vertexai=True)
# VERTEX_AI_PROJECT=your-gcp-project-id
# VERTEX_AI_LOCATION=us-central1

# =============================================================================
# OPENAI (Optional - for GPT models)
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
# Required only if using OpenAI models (gpt-4o, etc.)
# OPENAI_API_KEY=your-openai-api-key-here

# =============================================================================
# OLLAMA (Local LLM - No API key required)
# =============================================================================
# Ollama runs locally and doesn't require an API key
# Configure the base URL if Ollama is not running on default port
# OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# OPTIONAL: LangExtract Configuration
# =============================================================================
# Default model to use (optional, can be overridden in code)
# LANGEXTRACT_DEFAULT_MODEL=gemini-2.5-flash

# Debug mode (optional)
# LANGEXTRACT_DEBUG=false
